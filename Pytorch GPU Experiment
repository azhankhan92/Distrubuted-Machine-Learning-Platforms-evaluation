import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import pynvml
import time

# Initialize GPU monitoring
pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)

def get_gpu_memory_usage():
    info = pynvml.nvmlDeviceGetMemoryInfo(handle)
    return info.used / 1024**2  # Convert to MB

# Define model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(64 * 14 * 14, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Hyperparameters
batch_size = 64
num_epochs = 200
learning_rate = 0.001

# Load data
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST(
        './data', train=True, download=True,
        transform=transforms.Compose([transforms.ToTensor()])
    ),
    batch_size=batch_size, shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(
        './data', train=False,
        transform=transforms.Compose([transforms.ToTensor()])
    ),
    batch_size=batch_size, shuffle=False
)

# Initialize model, loss, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
train_loss = []
gpu_usage = []
epoch_times = []

for epoch in range(num_epochs):
    model.train()
    start_time = time.time()
    running_loss = 0.0

    for batch_idx, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)  # Move to GPU if available

        # Ensure batch sizes are consistent
        assert images.size(0) == labels.size(0), f"Mismatch: {images.size(0)} vs {labels.size(0)}"

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    end_time = time.time()
    epoch_time = end_time - start_time
    epoch_times.append(epoch_time)

    avg_loss = running_loss / len(train_loader)
    train_loss.append(avg_loss)
    gpu_usage.append(get_gpu_memory_usage())

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s, GPU Memory: {gpu_usage[-1]:.2f} MB")

# Plotting results
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_loss, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), epoch_times, label='Epoch Time')
plt.xlabel('Epoch')
plt.ylabel('Time (s)')
plt.title('Epoch Times')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), gpu_usage, label='GPU Memory Usage', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Memory Usage (MB)')
plt.title('GPU Memory Usage Over Epochs')
plt.legend()
plt.show()
import time
import torch
import random

# Simulated failure tolerance test
def simulate_failure_tolerance(model, test_loader, device):
    print("\n=== Simulating Failure Tolerance ===")
    fault_recovery_times = []
    recovery_attempts = 5  # Number of faults to simulate

    for i in range(recovery_attempts):
        try:
            print(f"\nSimulating failure {i + 1}/{recovery_attempts}...")
            # Randomly induce a "failure" by skipping a batch
            for idx, (images, labels) in enumerate(test_loader):
                if idx == random.randint(0, len(test_loader) - 1):
                    print("Fault: Batch processing skipped!")
                    raise ValueError("Simulated fault during batch processing.")

                # Perform a forward pass
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)

            print("Processing completed without further faults.")
        except ValueError as e:
            print(f"Fault encountered: {str(e)}")
            start_time = time.time()
            # Simulated recovery process
            time.sleep(0.5)  # Simulating recovery time
            fault_recovery_times.append(time.time() - start_time)
            print("Recovery successful!")

    print("\nRecovery times:", fault_recovery_times)
    return fault_recovery_times

# Simulated network latency and throughput
def simulate_network_metrics(dataset_size, epochs):
    print("\n=== Simulating Network Metrics ===")
    latency_per_epoch = random.uniform(0.05, 0.1)  # Simulated latency in seconds per epoch
    throughput = dataset_size / (latency_per_epoch * epochs)  # Samples processed per second

    print(f"Simulated network latency per epoch: {latency_per_epoch:.4f} seconds")
    print(f"Simulated throughput: {throughput:.2f} samples/sec")
    return latency_per_epoch, throughput

# Example usage
model.eval()  # Ensure the model is in evaluation mode

# Simulate failure tolerance during testing
fault_recovery_times = simulate_failure_tolerance(model, test_loader, device)

# Simulate network metrics for a dataset of size 60,000 and 200 epochs
latency_per_epoch, throughput = simulate_network_metrics(dataset_size=60000, epochs=200)

# Plotting fault recovery times
plt.figure(figsize=(6, 4))
plt.bar(range(len(fault_recovery_times)), fault_recovery_times, color='orange')
plt.xlabel("Recovery Attempt")
plt.ylabel("Recovery Time (s)")
plt.title("Fault Recovery Times")
plt.show()


